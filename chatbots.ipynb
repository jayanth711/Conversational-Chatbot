{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building A Chatbot\n",
    "\n",
    "conversational RAG:- Enable a chatbot experience over an external source of data\n",
    "Agents:- Build a Chatbot that can take actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  ## aloading all the environment variable\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000024D53B6C1F0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000024D53B6CEB0>, model_name='Gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatGroq(model=\"Gemma2-9b-it\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It's great that you enjoyed Salaar!  \\n\\nIt sounds like you're really enthusiastic about it.  What did you like most about the movie? The action, the story, the characters?  \\n\\nI'd love to hear more about what made Salaar so enjoyable for you! üòä\\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 25, 'total_tokens': 91, 'completion_time': 0.12, 'prompt_time': 0.000146989, 'queue_time': 0.014211911, 'total_time': 0.120146989}, 'model_name': 'Gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-b69f8967-686f-45a6-bf9e-ccd70917633e-0', usage_metadata={'input_tokens': 25, 'output_tokens': 66, 'total_tokens': 91})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hello, i watched Devera and salaar movie, buti like salaar\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You said you watched **Devera** and **Salaar**.  \\n\\nWhich one did you like better?  üòä \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 111, 'total_tokens': 139, 'completion_time': 0.050909091, 'prompt_time': 0.003516639, 'queue_time': 0.009818501, 'total_time': 0.05442573}, 'model_name': 'Gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-31df6df3-5f12-4746-b3b0-e271e6d18342-0', usage_metadata={'input_tokens': 111, 'output_tokens': 28, 'total_tokens': 139})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hello, i watched Devera and salaar movie, buti like salaa\"),\n",
    "        AIMessage(content='That\\'s great!  \\n\\nIs there anything specific you liked about the movie \"Devera\"? \\n\\nFor example, did you enjoy:\\n\\n* **The story?**\\n* **The acting?**\\n* **The music?** \\n* **The setting?**\\n\\n\\nTell me more about what made you like it! üòä \\n'),\n",
    "        HumanMessage(content=\"what movie i watched\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message History\n",
    "we can use message history class to wrap our model and make it stateful. This will help to keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store={}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]= ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config= {\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hello, i watched Devera and salaar movie, but i like salaar\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ah, I see! It sounds like you had a great time watching both \"Devera\" and \"Salaar\". It\\'s awesome that you have a favorite though! \\n\\nWhat is it about \"Salaar\" that you particularly enjoyed? The action, the story, the characters?  \\n\\nI\\'m always interested to hear what people think about movies!  üé¨  \\n\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You said you liked the movie **Salaar**!  \\n\\nIs there something specific about Salaar that you enjoyed?  Maybe a particular scene, character, or the overall story?  I'd love to hear more about what made it your favorite. üòä  \\n\\n\\n\\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 346, 'total_tokens': 404, 'completion_time': 0.105454545, 'prompt_time': 0.010717953, 'queue_time': 0.002718247, 'total_time': 0.116172498}, 'model_name': 'Gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-1a987390-80b2-4cf8-8bd1-788cec3058a0-0', usage_metadata={'input_tokens': 346, 'output_tokens': 58, 'total_tokens': 404})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content=\"which movie i like\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I have no memory of past conversations or any information about what you've watched. \\n\\nTo help me figure out which movie you're thinking of, tell me some details, like:\\n\\n* **What genre was it?** (comedy, action, horror, etc.)\\n* **When did you watch it?** (recently, a few years ago, when you were a kid?)\\n* **Who were the main actors?**\\n* **What was the plot about?** (even a vague description helps!)\\n\\n\\nThe more information you give me, the better chance I have of guessing the movie! üçø \\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### change the config ----> session ID\n",
    "config1= {\"configurable\":{\"session_id\":\"chat2\"}}\n",
    "response= with_message_history.invoke(\n",
    "    [HumanMessage(content=\"which movie i watched\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Jay! It's nice to meet you. üòÑ\\n\\nIs there anything you'd like to talk about or do today? I can tell you a story, answer your questions, or even help you write something. Just let me know!  \\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response= with_message_history.invoke(\n",
    "    [HumanMessage(content=\"MY NAME IS JAY\")],\n",
    "    config=config1\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Templates\n",
    "\n",
    "prompt template help to run user info into a formate that the LLM can work with. in thsi case, the raw user input is just a message. which we are passing to LLM. lets now make that a bit more complicated. 1st lets add in a system message with some custom instructions(but still taking message as input). Next, we'll add in more input besides just the messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"you are Python developre answer the coding questions accordingly\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "chain= prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hey Jay! üëã\\n\\nIt's great to meet you. What can I help you with today? \\n\\nDo you have a Python coding question you'd like me to answer? I'm ready to help you with anything from basic syntax to more complex concepts. üòä \\n\\nJust provide me with the question and any relevant code snippets, and I'll do my best to assist you!  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 25, 'total_tokens': 111, 'completion_time': 0.156363636, 'prompt_time': 0.000177289, 'queue_time': 0.014231881, 'total_time': 0.156540925}, 'model_name': 'Gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-098f7680-9b50-4270-ad5b-9686105bef9f-0', usage_metadata={'input_tokens': 25, 'output_tokens': 86, 'total_tokens': 111})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi! my name is jay\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history= RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Jay! \\n\\nIt seems like you're excited to be here!  \\n\\nI'm ready to answer your Python coding questions.  Just ask away, and I'll do my best to help. üòä \\n\\nWhat can I help you with today? üêç \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 25, 'total_tokens': 87, 'completion_time': 0.112727273, 'prompt_time': 0.000144359, 'queue_time': 0.01422822, 'total_time': 0.112871632}, 'model_name': 'Gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e39b3a9d-44d1-4284-9ce0-cf0e0c7fc8d2-0', usage_metadata={'input_tokens': 25, 'output_tokens': 62, 'total_tokens': 87})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config= {\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response= with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! my name is jay\")],\n",
    "    config=config\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add more complexity\n",
    "\n",
    "prompt= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"you are a python developer, answer the coding questions in {language}\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\n# Get input from the user\\nnum1 = float(input(\"Enter the first number: \"))\\nnum2 = float(input(\"Enter the second number: \"))\\n\\n# Calculate the product\\nproduct = num1 * num2\\n\\n# Display the result\\nprint(\"The product of\", num1, \"and\", num2, \"is:\", product)\\n```\\n\\n**Explanation:**\\n\\n1. **Get Input:**\\n   - `input(\"Enter the first number: \")` prompts the user to enter the first number and stores it as a string.\\n   - `float(...)` converts the input string to a floating-point number to handle potential decimal values.\\n\\n2. **Calculate Product:**\\n   - `product = num1 * num2` multiplies the two numbers and stores the result in the `product` variable.\\n\\n3. **Display Result:**\\n   - `print(\"The product of\", num1, \"and\", num2, \"is:\", product)` prints a user-friendly message displaying the original numbers and their product.\\n\\n**How to Run:**\\n\\n1. Save the code as a `.py` file (e.g., `multiplication.py`).\\n2. Open a terminal or command prompt.\\n3. Navigate to the directory where you saved the file.\\n4. Run the script using `python multiplication.py`.\\n\\nThe program will then ask you to enter two numbers, calculate their product, and display the result.\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"multiplication of 2 numbers\")],\"language\":\"python\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now wrap this more complicated chain in a meassage history calss. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history= RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\ndef multiply_numbers(num1, num2):\\n  \"\"\"\\n  Multiplies two numbers and returns the product.\\n\\n  Args:\\n    num1: The first number.\\n    num2: The second number.\\n\\n  Returns:\\n    The product of num1 and num2.\\n  \"\"\"\\n  return num1 * num2\\n\\n# Get the numbers from the user\\nnum1 = float(input(\"Enter the first number: \"))\\nnum2 = float(input(\"Enter the second number: \"))\\n\\n# Multiply the numbers and print the result\\nproduct = multiply_numbers(num1, num2)\\nprint(f\"The product of {num1} and {num2} is: {product}\")\\n```\\n\\n**Explanation:**\\n\\n1. **`multiply_numbers(num1, num2)` Function:**\\n   - Takes two numbers (`num1` and `num2`) as input.\\n   - Uses the `*` operator to multiply the two numbers.\\n   - Returns the result of the multiplication.\\n\\n2. **Main Part:**\\n   - Prompts the user to enter the first and second numbers using `input()`.\\n   - Converts the inputs to floating-point numbers using `float()` to allow for decimal values.\\n   - Calls the `multiply_numbers()` function to calculate the product.\\n   - Stores the result in the `product` variable.\\n   - Prints a clear message displaying the original numbers and their product.\\n\\n\\n\\nNow you can run this code, enter two numbers, and it will calculate and show you their product!\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config= {\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response= with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"write mutliplication numbers code\")], \"language\":\"python\"},\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing the conversation history\n",
    "\n",
    "one important concept to understand when building chatbot is how to manage conversation history. if left unmanaged the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "\n",
    "'trim_messages'    The term \"trim\" often means to remove extra spaces or characters. In programming, a trim_messages function might remove unnecessary whitespace from messages or limit their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're-a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 +2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "trimmer=trim_messages(\n",
    "    max_tokens= 45,\n",
    "    strategy= \"last\",\n",
    "    token_counter= model,\n",
    "    include_system=True,\n",
    "    allow_partial = False,\n",
    "    start_on= \"human\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're-a good assistant\"),\n",
    "    HumanMessage(content=\"hi! Im bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 +2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "\n",
    "]\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I have no memory of past conversations and don't know your preferences.  \\n\\nWhat's your favorite ice cream flavor? üòä  \\n\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain= (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"what ice cream do i like?\")],\n",
    "    \"language\":\"english\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked me \"whats 2 + 2\"  üòä \\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"what math problem did i asked you?\")],\n",
    "    \"language\":\"english\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets wrap in the message history\n",
    "with_message_history= RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config={\"configurable\":{\"session_id\":\"chat\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to past conversations or any personal information about you, including your name.  If you'd like to tell me your name, I'd be happy to know! üòä  \\n\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"what is my name?\")],\n",
    "    \"language\":\"english\"\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As a large language model, I have no memory of past conversations.  \\n\\nPlease provide the math problem you'd like me to help with, and I'll do my best to solve it! üòä  \\n\\n\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\":[HumanMessage(content=\"what math problem did i asked you?\")],\n",
    "        \"language\":\"english\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
